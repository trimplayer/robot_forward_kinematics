**LSTM**


Вкратце, архитектура LSTM (Long Short-Term Memory) представляет собой модификацию классической рекуррентной нейронной сети, созданную для эффективного моделирования долгосрочных зависимостей в последовательных данных. Основной блок LSTM, называемый «ячейкой» (cell), содержит три управляющих «ворот» (gates) — забывания, входа и выхода — и внутреннее состояние (cell state), которое обеспечивает пропуск информации без искажений на произвольные расстояния во времени. Ворота, реализованные в виде небольших полносвязных слоёв с сигмоидальной активацией, решают, какие фрагменты информации следует добавить, удалить или выдать на каждом временном шаге, тем самым позволяя сохранять значимые признаки и одновременно избегать проблем исчезающих или взрывающихся градиентов.

---

### **VanillaRNN и недостатки**

Архитектура VanillaRNN может связывать предыдущую информацию стекущей задачей. В этой архитектуре на вход принимаются как новый вектор, так и предыдущее скрытое состояние.
![RNN-longtermdependencies.png](RNN-longtermdependencies.png)

LSTM был разработан для преодоления ограничений vanillaRNN, главным образом проблемы угасающих градиентов при обучении на длинных последовательностях. В VanillaRNN сигнал ошибки при обратном распространении может со временем затухать, что затрудняет обучение долгосрочных зависимостей. Ядро LSTM решает эту проблему введением специального «конвейера» (cell state), по которому информация может течь практически без изменений, за исключением точечно управляемых операций.

---

##  **Архитектура LSTM**

![LSTM3-chain.png](LSTM3-chain.png)

### 1. Cell state (состояние ячейки)

Cell state – это вектор, проходящий через всю последовательность шагов LSTM практически без изменений, что обеспечивает сохранность информации на длительные промежутки времени. Минимальные линейные трансформации этого состояния предотвращают искажение градиентов, делая возможным обучение долговременных зависимостей.

### 2. Ворота (Gates)

Каждый LSTM-блок содержит три гейта: forget, input, output. Они реализованны как небольшие полносвязные слои с сигмоидной активацией, которые вычисляют значения в диапазоне \[0,1]:

* **Ворота забывания (Forget Gate)** определяют, какую часть предыдущего состояния удалить из cell state. Функция:
  $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
  где $h_{t-1}$ — скрытое состояние предыдущего шага, $x_t$ — текущий вход.

* **Ворота входа (Input Gate)** решают, какую новую информацию добавить в cell state. Состоит из двух частей:

  - Сигмоидальный слой:
     $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
  - Кандидат-состояние:
     $\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$.

* **Ворота выхода (Output Gate)** контролируют, какую часть cell state перенести в скрытое состояние $h_t$ и выдавать на следующий слой или шаг:   
  $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
  $h_t = o_t * \tanh(C_t)$.


## 3. Математическая формализация

Полный набор уравнений LSTM на шаге $t$:

1. $f_t = \sigma(W_f[h_{t-1}, x_t] + b_f)$
2. $i_t = \sigma(W_i[h_{t-1}, x_t] + b_i)$
3. $\tilde{C}_t = \tanh(W_C[h_{t-1}, x_t] + b_C)$
4. $C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$
5. $o_t = \sigma(W_o[h_{t-1}, x_t] + b_o)$
6. $h_t = o_t \odot \tanh(C_t)$.


---

## **Варианты и расширения**

Существуют расширения классического LSTM:

* **Peephole LSTM**, где ворота имеют прямой доступ к предыдущему состоянию ячейки.
* **Coupled Input-Forget Gates**, объединяющие входные и забывающие ворота для сокращения числа параметров.
* **Bidirectional LSTM**, обрабатывающий последовательность в прямом и обратном направлении для учёта контекста с обеих сторон.

## **Применения**

LSTM широко используются в задачах обработки естественного языка (машинный перевод, генерация текста), распознавания речи, анализа временных рядов, а также в робототехнике и финансовом прогнозировании. Её способность улавливать долгосрочные зависимости делает её краеугольным камнем моделирования последовательностей.

---

Таким образом, архитектура LSTM сочетает в себе «ячейку» для хранения информации и «ворота» для выборочного управления потоком данных, что позволяет эффективно моделировать зависимости на больших расстояниях во временных последовательностях.


[//]: # (https://colah.github.io/posts/2015-08-Understanding-LSTMs/?utm_source=chatgpt.com "Understanding LSTM Networks - colah's blog")
[//]: # (original article: https://deeplearning.cs.cmu.edu/S23/document/readings/LSTM.pdf)

